# **TAIYÅŒAI INC.**

## **Data Engineering Trial Task**

### **Objective**

The primary goal of this task is to identify, standardize, and continuously update data related to construction and infrastructure projects and tenders within the state of California.

### **Part 1: Research and Data Sourcing**

#### **Task**
Research and identify 5-10 reliable data sources pertaining to construction and infrastructure projects and tenders in California.

#### **Methodology**
Utilize a combination of online research and advanced language models (e.g., OpenAI's GPT models) to identify relevant data sources. Clearly document the research process, explaining how GPT or similar models were employed to enhance the research and why they were chosen for this task.

### **Part 2: Data Extraction and Standardization**

#### **Task**
Based on the provided Table 1 and the sources you identified, propose and implement methods to scrape data using language model-based tools like OpenAI API, Mistral 7B, Llama2, or other open-source models.

#### **Requirements**
- Develop data products (DPs) capable of efficiently scraping data from multiple sources.
- Standardize the scraped data following the guidelines outlined in Table 2.

### **Part 3: Automation and Continuous Updating**

#### **Task**
Design and propose a system that automates the data scraping and standardization processes, ensuring the system can be continuously updated with minimal manual intervention.

#### **Details**
- Describe the mechanisms by which the data sources will be regularly updated.
- Detail the use of cron jobs or similar scheduling tools to automate ongoing data updates, emphasizing adherence to production environment standards.

### **Evaluation Criteria**

- **Scalability**: Demonstrate the ability to scrape and manage data from multiple sources effectively.
- **Adherence to Standards**: Ensure all methods and outputs conform to the provided data standards. Penalties will apply for any deviations.
- **Automation and Continuity**: Present a robust strategy for continuous data updating, including the implementation of cron monitoring and the suitability of the approach for a production environment.

### **Deliverables**

Candidates are expected to submit a Google Drive folder containing the following:

1. **Python Scripts**: The code used for data scraping and standardization.
2. **Documentation**: Comprehensive explanations of the scripts and methodologies used.
3. **Sample Datasets**: Examples of the extracted and standardized data.
4. **Production Environment Plan**: A document outlining the implementation of cron monitoring and the operational strategy for a production environment.

### **Notes to Candidates**

- Ensure your methods are scalable and suitable for deployment in a production environment.
- Clearly explain how AI or machine learning models were used in data sourcing and preprocessing tasks.
- Provide a well-considered approach to the continuous updating and monitoring of data, demonstrating long-term viability.
